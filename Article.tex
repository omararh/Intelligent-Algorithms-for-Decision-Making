\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}

\title{Algorithmes Génétiques}
\author{Omar Arharbi}
\date{Janvier 2025}

\begin{document}

\maketitle

\tableofcontents  % Ajout de la table des matières
\newpage  % Saut de page pour que la table des matières soit bien séparée du contenu

\section{Introduction}
\subsection{Algorithmes évolutionnaires}

Les algorithmes évolutionnaires représentent une approche bio-inspirée de l'optimisation. Ils constituent une méthode d'optimisation innovante, puisant son inspiration dans les principes de l'évolution biologique. Cette approche vise à résoudre des problèmes complexes en simulant le processus d'évolution naturelle.

\subsubsection{Processus itératif}

L'algorithme fonctionne de manière itérative, suivant ces étapes clés :

\begin{itemize}
    \item Initialisation : Création d'une population initiale diverse.
    \item Évaluation : Mesure de la qualité (fitness) de chaque individu.
    \item Sélection : Choix des individus les plus prometteurs.
    \item Reproduction : Création de nouveaux individus par croisement.
    \item Mutation : Introduction de variations aléatoires.
    \item Remplacement : Intégration des nouveaux individus dans la population.
\end{itemize}

\subsubsection{Opérateurs clés}
\begin{itemize}
    \item Sélection : Identifie les meilleurs individus, souvent par des méthodes comme la \item roulette ou le tournoi.
    \item Croisement : Combine les caractéristiques de deux parents pour créer des descendants.
    \item Mutation : Apporte des modifications aléatoires pour maintenir la diversité.
\end{itemize}

\subsubsection{Objectif et arrêt}
L'algorithme vise à maximiser une fonction objective, représentant la qualité des solutions. Il s'arrête lorsqu'un critère prédéfini est atteint, comme un nombre maximal d'itérations ou l'obtention d'une solution satisfaisante.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{The-basic-process-of-genetic-algorithm.png}
    \caption{Genetic algorithm process}
    \label{fig:enter-label}
\end{figure}

\subsubsection{Équilibre crucial}
 Le succès de l'algorithme repose sur un équilibre délicat entre l'exploitation des meilleures solutions trouvées et l'exploration de nouvelles possibilités, guidé par le choix judicieux des opérateurs.


\subsection{Le problème One-Max}

Le problème One-Max est un exemple classique utilisé pour illustrer le fonctionnement des algorithmes génétiques. Il se caractérise par sa simplicité et son efficacité pédagogique.

\subsubsection{Définition du problème}

Soit une liste binaire $L$ de longueur $N$, où chaque élément $l_i \in \{0,1\}$.

La fonction de fitness $f(L)$ est définie comme :

\[
f(L) = \sum_{i=1}^N l_i
\]

\subsubsection{Objectif}

L'objectif est de maximiser la valeur de fitness, c'est-à-dire trouver la liste $L$ qui produit la plus grande somme possible.

\subsubsection{Exemple}

Pour $N = 10$, considérons la liste suivante générée aléatoirement :

\[
L = [0, 1, 0, 0, 1, 1, 1, 0, 0, 0]
\]

Sa valeur de fitness est :

\[
f(L) = 0 + 1 + 0 + 0 + 1 + 1 + 1 + 0 + 0 + 0 = 4
\]

\subsubsection{Solution optimale}

La solution optimale pour $N = 10$ est :

\[
L_{opt} = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
\]

avec une fitness maximale :

\[
f(L_{opt}) = 10
\]

\subsubsection{Rôle de l'algorithme génétique}

Un algorithme génétique efficace devrait converger vers cette solution optimale, produisant des individus (listes) composés uniquement de 1, atteignant ainsi la fitness maximale égale à $N$.

\section{Présentation des trois méthodes étudiées}
Afin de résoudre le problème du One-Max, nous avons implémenté différents algorithmes évolutionnaires.

\subsection{Algorithme génétique steady state}
L'algorithme steady state est conçu pour choisir un nombre spécifique d'individus à chaque génération, généralement deux, mais ce nombre peut être étendu. L'initialisation de la population peut se faire de manière fixe ou aléatoire.

\subsubsection{Méthodes de sélection}

Pour sélectionner N individus, plusieurs approches sont possibles :

\begin{itemize}
    \item \textbf{Sélection par roulette :} La probabilité de sélection est proportionnelle à la valeur de fitness.
    \item \textbf{Sélection par tournoi :} Des tournois de taille k sont organisés, les vainqueurs étant sélectionnés.
    \item \textbf{Sélection par classement :} Les N individus ayant les meilleures performances sont retenus.
    \item \textbf{Sélection aléatoire :} N individus sont choisis au hasard dans la population.
\end{itemize}

\subsubsection{Techniques de croisement}

Les individus sélectionnés subissent un croisement selon diverses méthodes :

\begin{itemize}
    \item \textbf{Croisement mono-point :} Un point d'échange est déterminé aléatoirement.
    \item \textbf{Croisement bi-point :} Deux points d'échange sont choisis au hasard.
    \item \textbf{Croisement uniforme :} Chaque caractéristique est héritée indépendamment d'un parent ou de l'autre.
\end{itemize}

\subsubsection{Opérations de mutation}

Après le croisement, chaque individu a une probabilité P de subir une mutation :

\begin{itemize}
    \item \textbf{Mutation bit-flip :} Chaque bit peut être modifié avec une probabilité P.
    \item \textbf{Mutation one-flip :} Un seul bit est modifié aléatoirement.
    \item \textbf{Mutation k-flip :} k bits sont sélectionnés et modifiés au hasard.
\end{itemize}

\subsubsection{Renouvellement de la population}

L'intégration des nouveaux individus dans la population existante peut se baser sur différents critères, tels que l'ancienneté ou la performance des individus.

\subsubsection{Critères d'arrêt}

L'algorithme se termine selon des conditions prédéfinies, comme un nombre maximal de générations ou l'atteinte d'une solution satisfaisante.


\subsubsection{Pseudo code}

\includegraphics[width=0.9\linewidth]{steady-pseudo-algo.png}

\subsection{Algorithme à estimation de distribution}
\subsubsection{Définition}
L'algorithme à estimation de distribution vise à estimer la distribution de probabilité des solutions dans l'espace de recherche.

\subsubsection{Exemple}
Voici un exemple détaillé pour N = 4 et K = 3 :

\begin{enumerate}
\item Vecteur de distribution initiale : [0.5, 0.3, 0.7, 0.4]

\item Population initiale générée :
    \begin{itemize}
    \item Individu 1: [1, 0, 1, 0] → Fitness = 2
    \item Individu 2: [1, 1, 1, 0] → Fitness = 3
    \item Individu 3: [0, 0, 1, 1] → Fitness = 2
    \item Individu 4: [1, 0, 1, 1] → Fitness = 3
    \item Individu 5: [1, 1, 1, 1] → Fitness = 4
    \end{itemize}

\item Sélection des K = 3 meilleurs individus :
    \begin{itemize}
    \item [1, 1, 1, 1] → Fitness = 4
    \item [1, 1, 1, 0] → Fitness = 3
    \item [1, 0, 1, 1] → Fitness = 3
    \end{itemize}

\item Calcul du nouveau vecteur de distribution :
    \begin{itemize}
    \item Position 1: (1 + 1 + 1)/3 = 1.0
    \item Position 2: (1 + 1 + 0)/3 = 0.66
    \item Position 3: (1 + 1 + 1)/3 = 1.0
    \item Position 4: (1 + 0 + 1)/3 = 0.66
    \end{itemize}

\item Nouveau vecteur de distribution : [1.0, 0.66, 1.0, 0.66]

\item L'algorithme continue avec ce nouveau vecteur pour générer la prochaine population, et ainsi de suite jusqu'à atteindre le critère d'arrêt.
\end{enumerate}
On peut observer que le vecteur de probabilité évolue pour favoriser les bits qui apparaissent fréquemment dans les meilleures solutions.

\subsubsection{Pseudo code}

\includegraphics[width=0.9\linewidth]{Estim-distrib-algo.png}

\subsection{Algorithme compact}
\subsubsection{Définition}
L'algorithme génétique compact (cGA) est une variante minimaliste des algorithmes d'estimation de distribution. Il opère sur un vecteur de probabilité $V$ de taille $N$ initialisé uniformément à 0.5, représentant la probabilité de chaque bit d'être à 1. À chaque génération, seuls deux individus sont générés à partir de ce vecteur, puis comparés pour mettre à jour les probabilités.

La mise à jour du vecteur se fait en comparant le meilleur individu (winner) et le moins bon (loser). Pour chaque position $i$ :
\begin{itemize}
    \item Si les bits sont identiques, $V[i]$ reste inchangé
    \item Si $\text{winner}[i] = 1$ et $\text{loser}[i] = 0$ : $V[i] \leftarrow V[i] + \frac{1}{N}$
    \item Si $\text{winner}[i] = 0$ et $\text{loser}[i] = 1$ : $V[i] \leftarrow V[i] - \frac{1}{N}$
\end{itemize}

\subsubsection{Exemple}
Pour $N = 6$ :

\begin{enumerate}
   \item Vecteur initial : $V = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$

   \item Population générée :
   \begin{itemize}
       \item Individu 1 : [1, 1, 0, 0, 1, 1] → Fitness = 4
       \item Individu 2 : [0, 1, 0, 1, 0, 0] → Fitness = 2
   \end{itemize}

   \item Comparaison bit à bit (Ind1 = winner, Ind2 = loser):
   \begin{itemize}
       \item Position 1 : 1 vs 0 → $V[1] = 0.5 + \frac{1}{6}$
       \item Position 2 : 1 vs 1 → $V[2]$ inchangé
       \item Position 3 : 0 vs 0 → $V[3]$ inchangé
       \item Position 4 : 0 vs 1 → $V[4] = 0.5 - \frac{1}{6}$
       \item Position 5 : 1 vs 0 → $V[5] = 0.5 + \frac{1}{6}$
       \item Position 6 : 1 vs 0 → $V[6] = 0.5 + \frac{1}{6}$
   \end{itemize}

   \item Nouveau vecteur : $V = [0.67, 0.5, 0.5, 0.33, 0.67, 0.67]$
\end{enumerate}

Ce processus est répété jusqu'à convergence ou jusqu'à ce qu'un critère d'arrêt soit atteint.

\subsubsection{Diagramme et Pseudo code}

\includegraphics[width=0.9\linewidth]{diagram-compact.png}

\includegraphics[width=0.9\linewidth]{compact-pseudo-algo.png}

\subsection{Comparaison/Différences/ressemblances entre les approches}

\includegraphics[width=1.2\linewidth]{image.png}

Ces trois algorithmes partagent l'objectif d'optimisation évolutionnaire mais diffèrent dans leur approche de gestion de la population. L'algorithme génétique steady-state est le plus "classique" : il maintient une population constante et ne remplace que quelques individus à chaque génération via les opérateurs traditionnels (croisement et mutation). L'algorithme génétique compact (cGA) représente l'autre extrême : il simule une population via un simple vecteur de probabilités, sans maintenir de population réelle, ce qui le rend très économe en mémoire. L'algorithme à estimation de distribution (EDA) se situe entre les deux : comme le cGA, il utilise un modèle probabiliste, mais il est plus sophistiqué car il capture les dépendances entre les variables et maintient une vraie population. Le steady-state modifie sa population progressivement, tandis que l'EDA génère une population entièrement nouvelle à chaque génération en échantillonnant son modèle probabiliste. Le cGA peut être vu comme un cas particulier d'EDA avec une population minimale et un modèle probabiliste très simple.

\section{Analyse des Algorithmes Génétiques}

Pour évaluer l'efficacité et le comportement des algorithmes génétiques, nous adoptons une approche méthodique consistant à modifier un seul paramètre à la fois. Cette méthode nous permet d'isoler l'impact spécifique de chaque variable sur les performances de l'algorithme. Afin d'établir une base de comparaison cohérente, nous avons défini un ensemble de paramètres par défaut :

\begin{itemize}
    \item \textbf{Structure des individus :} Chaque individu est représenté par une chaîne de 300 bits.
    \item \textbf{Taille de la population :} L'algorithme opère sur un ensemble de 20 individus.
    \item \textbf{Opérations génétiques :}
    \begin{itemize}
        \item Le croisement est systématiquement appliqué (probabilité de 100\%).
        \item La mutation est également appliquée à chaque génération (probabilité de 100\%).
    \end{itemize}
    \item \textbf{Robustesse statistique :} Chaque configuration est exécutée 30 fois pour assurer la fiabilité des résultats.
    \item \textbf{Génération initiale :} La population de départ est créée de manière aléatoire.
    \item \textbf{Sélection parentale :} À chaque itération, deux individus sont choisis pour la reproduction.
    \item \textbf{Opérateurs génétiques par défaut :}
    \begin{itemize}
        \item \textit{Sélection :} Tournoi impliquant 3 individus.
        \item \textit{Croisement :} Méthode uniforme.
        \item \textit{Mutation :} Technique du "bit-flip".
    \end{itemize}
    \item \textbf{Stratégie de remplacement :} Les nouveaux individus remplacent les moins performants de la génération précédente, favorisant ainsi le renouvellement basé sur la qualité plutôt que sur l'âge.
\end{itemize}

Ces paramètres constituent notre configuration de référence. Dans notre analyse, nous ferons varier un seul de ces paramètres à la fois, tout en maintenant les autres constants. Cette approche nous permettra d'évaluer précisément l'influence de chaque facteur sur les performances globales de l'algorithme génétique.

\subsection{Algorithme steady state}
\subsubsection{la taille de la population}

\includegraphics[width=0.9\linewidth]{taille-population.png}

 L'analyse des résultats révèle une corrélation inverse entre la taille de la population et la vitesse de convergence vers une solution optimale. Les populations de taille réduite tendent à atteindre plus rapidement un point de convergence. Ce phénomène s'explique par le processus de sélection : dans un groupe restreint, la probabilité de choisir les individus les plus performants augmente significativement. Par conséquent, les caractéristiques génétiques favorables se propagent plus efficacement aux générations subséquentes.

\subsubsection{Méthodes de séléction}
\includegraphics[width=0.9\linewidth]{selection-method.png}

\begin{itemize}
    \item \textbf{Sélection des Meilleurs} montre la convergence la plus rapide, ce qui est logique car elle sélectionne systématiquement les meilleurs individus. Cependant, cette approche pourrait limiter la diversité génétique.

    \item \textbf{Sélection par Tournoi} offre une performance intermédiaire avec une convergence régulière. Elle maintient un bon équilibre entre exploitation des bonnes solutions et exploration.

    \item \textbf{Sélection par Roulette} et \textbf{Sélection Aléatoire} montrent des performances similaires et plus lentes, mais finissent par atteindre des résultats comparables aux autres méthodes après 1500 générations.
\end{itemize}

\paragraph{Conclusion} Si l'objectif est d'obtenir rapidement de bons résultats, la sélection des meilleurs est préférable. Cependant, la sélection par tournoi offre un meilleur compromis entre vitesse de convergence et maintien de la diversité. Les méthodes de roulette et aléatoire, bien que plus lentes, peuvent être utiles dans des problèmes où l'exploration de l'espace de recherche est cruciale.

\subsubsection{Méthodes de croisement}
\includegraphics[width=0.9\linewidth]{crossOver-methods.png}

\begin{itemize}
    \item \textbf{Croisement Uniforme} montre la convergence la plus rapide. Cette performance supérieur peut s'expliquer par sa capacité à échanger des bits de manière plus uniforme sur l'ensemble de la chaîne.

    \item \textbf{Croisement Deux Points} offre une performance intermédiaire, légèrement meilleure que le croisement un point mais moins efficace que le croisement uniforme.

    \item \textbf{Croisement Un Point} présente la convergence la plus lente, bien qu'il finisse par atteindre des résultats similaires aux autres méthodes après 1500 générations.
\end{itemize}

\paragraph{Conclusion} Le croisement uniforme apparaît comme la méthode la plus efficace pour ce problème, probablement parce qu'il permet une meilleure exploitation des bonnes solutions en offrant plus de possibilités de combinaisons entre les parents. Les croisements à un et deux points, bien que moins performants initialement, restent viables car ils atteignent finalement la même qualité de solution.

\subsubsection{Méthodes de mutations}

\includegraphics[width=0.9\linewidth]{mutation-methods.png}

\begin{itemize}
    \item \textbf{Un Flip} (mutation d'un seul bit) montre une convergence légèrement plus rapide jusqu'à la génération 750 et atteint la meilleure performance finale.

    \item \textbf{Bit Flip} (mutation probabiliste de chaque bit) présente une performance similaire à Un Flip, avec une convergence un peu plus lente initialement mais rattrapant son retard après 1250 générations.

    \item \textbf{Trois Flips} offre une performance intermédiaire, légèrement inférieure aux deux premières méthodes.

    \item \textbf{Cinq Flips} montre la performance la plus faible, avec un écart significatif par rapport aux autres méthodes qui se maintient jusqu'à la fin.
\end{itemize}

\paragraph{Conclusion} On observe une corrélation claire entre le nombre de bits mutés et la performance : plus le nombre de bits mutés est important, moins la performance est bonne. Cela suggère que des mutations trop importantes peuvent être déstabilisantes pour le problème OneMax. Les méthodes Un Flip et Bit Flip, qui modifient moins de bits en moyenne, semblent offrir le meilleur compromis entre exploration et exploitation.

\subsection{Algorithme à estimation de distribution}
L'algorithme à estimation de distribution offre une flexibilité intéressante dans le paramétrage du processus évolutif. Un aspect clé de cette flexibilité réside dans la possibilité d'ajuster le nombre d'individus considérés comme "élites" ou "parents" à chaque génération. Ce paramètre joue un rôle crucial dans la dynamique de l'algorithme, influençant directement la rapidité avec laquelle il converge vers une solution optimale.

Le mécanisme central de cet algorithme repose sur la mise à jour du vecteur de probabilité. Cette mise à jour s'effectue en calculant la moyenne des caractéristiques (bits) des individus sélectionnés comme parents. Ainsi, le choix du nombre de parents impacte directement la façon dont l'information génétique est transmise et exploitée au fil des générations.

Un autre paramètre d'intérêt est la taille globale de la population. Il est pertinent d'examiner comment les variations de cette taille interagissent avec le nombre d'individus sélectionnés comme parents. Cette analyse permet de comprendre si une population plus large nécessite proportionnellement plus de parents pour maintenir une diversité génétique optimale, ou si un petit groupe d'élites suffit, quelle que soit la taille de la population.

Ces considérations ouvrent la voie à une exploration approfondie des dynamiques de l'algorithme, permettant d'optimiser son efficacité en fonction des spécificités du problème à résoudre.

\subsubsection{Nombre de meilleures individus (K best)}
\includegraphics[width=0.9\linewidth]{NB-k-best-individus.png}

Avec une taille de population = 30, l'analyse des performances de l'algorithme à estimation de distribution révèle des dynamiques intéressantes en fonction du nombre d'individus sélectionnés comme parents. Dans le contexte d'une population standard de 30 individus, on constate que la sélection d'un groupe de 4 à 8 parents semble offrir les meilleurs résultats.

Lors des phases initiales de l'évolution, une sélection restreinte à quatre parents démontre une efficacité supérieur. Cette approche conservatrice minimise le risque d'inclure des individus sous-optimaux dans le pool génétique parental, assurant ainsi une convergence rapide vers des solutions prometteuses.

Cependant, à mesure que l'algorithme progresse, on observe une homogénéisation croissante de la population. Un nombre croissant d'individus atteint des niveaux de fitness proches du maximum observé. Dans ce contexte d'amélioration globale, l'élargissement du groupe parental à huit individus devient avantageux. Cette stratégie permet d'exploiter une plus grande diversité génétique tout en maintenant une qualité élevée des parents sélectionnés.


En revanche, l'extension du groupe parental à 14 individus s'avère contre-productive. Cette approche trop inclusive risque d'intégrer des individus de moindre qualité dans le processus de reproduction, diluant ainsi l'efficacité de la sélection et potentiellement ralentissant la convergence vers des solutions optimales.

\subsubsection{Augmentation de taille de la population}
\textbf{Taille = 90}

\includegraphics[width=0.9\linewidth]{taill-population.png}

On remarque ici qu'ici la selection de 14 parents devient plus intéressantes.

\subsection{Algorithme génétique compact}

Pour accélérer la convergence de l'algorithme, il est possible d'introduire un coefficient d'apprentissage $\alpha$. Par défaut, $\alpha = 1$, mais sa valeur peut être ajustée pour modifier la vitesse d'apprentissage. Avec ce coefficient, la formule de mise à jour devient :

\[
    \omega_i = \omega_i + \alpha \cdot \frac{1}{N} \cdot (winner_i - loser_i)
\]

Cette formulation permet un contrôle plus fin de l'évolution du vecteur de probabilité, offrant la possibilité d'ajuster la balance entre exploration et exploitation dans l'espace de recherche, et bien sur en fonction du $\alpha$ en modifie le nombre maixmum de génération.

\subsubsection{Taux d'apprentissage a 1 ($\alpha$ = 1)}
\includegraphics[width=0.9\linewidth]{compact-algo.png}

L'expérimentation montre l'évolution de la fitness sur 7000 générations avec deux métriques principales :

\begin{itemize}
    \item \textbf{Convergence globale :} L'algorithme atteint une fitness proche de 300 (l'optimum) après environ 7000 générations, démontrant sa capacité à trouver la solution optimale, mais la progression reste lente.

    \item \textbf{Écart fitness max/moyenne :} L'écart entre la fitness maximale et moyenne reste relativement constant tout au long de l'évolution, indiquant une bonne stabilité de l'algorithme et une bonne diversité de population.

    \item \textbf{oscillations :} Les oscillations observées dans les courbes sont dues à la mise à jour progressive du vecteur de probabilité, qui détermine la génération des individus. À chaque génération, le vecteur est ajusté en fonction du gagnant et du perdant de la sélection, mais ces mises à jour ne sont pas continues ni uniformes, ce qui entraîne des fluctuations locales dans la fitness. De plus, ici on ne maintient pas une population explicite mais on ajuste uniquement des probabilités.
\end{itemize}

\paragraph{Conclusion} L'algorithme montre une progression stable et constante vers l'optimum, caractéristique typique du cGA. La convergence est plus lente qu'un algorithme génétique classique et donc on a besoin d'un nombre de générations supérieur de ce qu'on a vu auparavant ce nombre qui peut diminuer en augementant le taux d'apprentissage $\alpha$.

\subsubsection{Augmentation du taux d'apprentissage ($\alpha$ = 2)}

\includegraphics[width=0.9\linewidth]{compact-augemente-alpha.png}

L'augmentation du taux d'apprentissage à 2 a permis une convergence plus rapide, réduisant le nombre de générations nécessaires pour atteindre la fitness maximale de 300 or ici on a eu besoin que de 3500 générations pour converger. Cependant, cette accélération entraîne des oscillations plus marquées en début de progression, dues aux mises à jour plus agressives du vecteur de probabilité. Globalement, le compromis entre vitesse de convergence et stabilité est bien visible, avec une convergence plus rapide mais des variations plus prononcées dans les premières phases de l'évolution.

\section{Sélection automatique de l’opérateur de mutation}

L'algorithme génétique steady state peut être optimisé en introduisant une sélection dynamique des opérateurs de mutation. Plutôt que d'utiliser exclusivement la mutation bit-flip par défaut, qui est sous-optimale dans les phases initiales de l'évolution, cette approche propose une adaptation en temps réel du choix de l'opérateur de mutation.

Cette amélioration permet à l'algorithme de sélectionner l'opérateur de mutation le plus approprié pour chaque individu, en tenant compte de son efficacité et de l'état actuel de la population. Pour mettre en œuvre cette stratégie adaptative, deux méthodes ont été développées : la roulette adaptative et l'algorithme UCB (Upper Confidence Bound).

Ces techniques d'adaptation dynamique visent à optimiser la performance de l'algorithme en ajustant continuellement la stratégie de mutation en fonction des résultats observés, permettant ainsi une exploration plus efficace de l'espace de recherche.

\subsection{Roulette adaptative}
\subsubsection{Définition}

La méthode de la roulette adaptative implémente une stratégie d'optimisation dynamique des opérateurs de mutation. Elle utilise un ensemble $\Omega = \{o_1, \ldots, o_n\}$, où chaque $o_i$ représente un opérateur de mutation distinct. À chaque opérateur est associée une probabilité de sélection, stockée dans un vecteur $\theta$.

L'efficacité $u^t_i$ d'un opérateur $o_i$ à l'instant $t$ est évaluée selon la formule :

\begin{equation}
    u^t_i = (1 - \alpha)u^{t-1}_i + \alpha \cdot g(o_i, s_0, \ldots, s_{t-1})
\end{equation}

où :
\begin{itemize}
    \item $\alpha$ est un facteur d'apprentissage
    \item $u^0_i = 0$ (valeur initiale)
    \item $g(o_i, s_0, \ldots, s_{t-1}) = \max(0, \text{nouvelle fitness} - \text{ancienne fitness})$ représente le gain de l'opérateur
\end{itemize}

La probabilité de sélection $\sigma^{t+1}_i$ d'un opérateur pour l'instant $t+1$ est calculée comme suit :

\begin{equation}
    \sigma^{t+1}_i = p_{\text{min}} + (1 - n \cdot p_{\text{min}}) \cdot \frac{u^{t+1}_i}{\sum_{k=1}^n u^{t+1}_k}
\end{equation}

où $p_{\text{min}}$ est une probabilité minimale garantie pour chaque opérateur.

\subsubsection{pseudo code}
\includegraphics[width=0.9\linewidth]{roulete-adaptive-pseudo-algo.png}

\subsection{Analyse expérimentale}

\subsubsection{Comparaison entre les différents opérateurs de mutation}
\includegraphics[width=0.9\linewidth]{roullette.png}

Dans l'évolution d'un algorithme génétique, les stratégies de mutation varient selon le stade de progression. Durant les 350 premières générations, les opérateurs 3-flip et 5-flip sont privilégiés en raison de leur capacité à introduire des changements significatifs. Après ce seuil, les opérateurs 1-flip et bit-flip deviennent plus efficaces, car la probabilité de modifier un bit à zéro devient plus faible, nécessitant des mutations plus précises et ciblées pour continuer l'optimisation.

\subsubsection{Comparaison de la roulette adaptative et la roulette fixe}
\includegraphics[width=0.9\linewidth]{analyse-roulette.png}

La comparaison entre la roulette adaptative (en violet) et la roulette fixe utilisant uniquement le bit-flip (en rouge) montre des performances intéressantes sur le problème OneMax avec des chaînes de 300 bits. La roulette adaptative démontre une convergence plus rapide dans les premières générations (0-1000), obtenant une meilleure fitness moyenne grace a son utilisation du 3-flip ou 5-flip au départ . Cependant, les deux approches finissent par atteindre des performances similaires vers la fin de l'évolution (autour de la génération 3000), avec une fitness moyenne proche de 300 (l'optimum pour ce problème). Cela suggère que bien que le bit-flip soit l'opérateur le plus efficace, l'utilisation adaptative de multiples opérateurs offre un avantage en termes de vitesse de convergence initiale.

\include{UCB-algo}

\end{document}
